apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: opentelemetry-kube-stack
  namespace: opentelemetry
spec:
  releaseName: opentelemetry-kube-stack
  chart:
    spec:
      chart: opentelemetry-kube-stack
      # renovate: datasource=helm depName=opentelemetry-kube-stack registryUrl=https://open-telemetry.github.io/opentelemetry-helm-charts
      version: "0.6.2"
      sourceRef:
        kind: HelmRepository
        name: open-telemetry
        namespace: flux-system
  interval: 50m
  install:
    remediation:
      retries: 3
  values:
    # https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-kube-stack/values.yaml
    crds:
      installOtel: true
      installPrometheus: true

    opentelemetry-operator:
      # Subchart values https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-operator/values.yaml
      enabled: true
      manager:
        collectorImage:
          repository: otel/opentelemetry-collector-contrib
      crds:
        create: false # Disable as we install the CRDs globally
      
      replicaCount: 2 # Number of hypervisors

      pdb:
        create: true
        minAvailable: 1
      
      topologySpreadConstraints:
        # Maximum 1 pod per Proxmox hypervisor
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: opentelemetry-operator
              app.kubernetes.io/component: controller-manager
        # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: opentelemetry-operator
              app.kubernetes.io/component: controller-manager

    defaultCRConfig:

      observability:
        metrics:
          enableMetrics: true

      config:
        service:
          # Otel debug logs
          telemetry:
            logs:
              level: DEBUG
          pipelines:
            logs:
              exporters: &victoriametrics-exporter [otlphttp/victoriametrics, debug]
            metrics:
              exporters: *victoriametrics-exporter
        
        exporters:
          otlphttp/victoriametrics:
            compression: gzip
            encoding: proto
            # URL format from https://docs.victoriametrics.com/victoriametrics/cluster-victoriametrics/#url-format
            metrics_endpoint: http://vminsert-metrics-datastore.monitoring.svc.cluster.local:8480/insert/0/opentelemetry/v1/metrics
            logs_endpoint: http://vlinsert-logs-datastore.monitoring.svc.cluster.local:9481/insert/opentelemetry/v1/logs
            tls:
              insecure: true

    collectors:
      daemon:
        enabled: true

        presets:
          logsCollection:
            enabled: true
            includeCollectorLogs: false
          kubeletMetrics:
            enabled: true
          hostMetrics:
            enabled: true
          kubernetesAttributes:
            enabled: true
        
        # Should run as root to collect hostMetrics
        securityContext:
          # readOnlyRootFilesystem:
          runAsNonRoot: false
          runAsUser: 0
          runAsGroup: 0

        # Tolerations to schedule daemon pods on control-plane nodes
        tolerations:
          - key: node-role.kubernetes.io/control-plane
            effect: NoSchedule

        config:
          service:
            pipelines:
              logs:
                exporters: *victoriametrics-exporter
              metrics:
                exporters: *victoriametrics-exporter

      cluster:
        enabled: true

        presets:
          kubernetesAttributes:
            enabled: true
          kubernetesEvents:
            enabled: true
          clusterMetrics:
            enabled: true
        

        # We wan't to keep a pod running
        podDisruptionBudget:
          minAvailable: 1
        
        config:
          receivers:
            # InfluxDB receiver for proxmox hypervisor metrics (tcp)
            influxdb:
              endpoint: 0.0.0.0:8086
            k8sobjects:
              auth_type: serviceAccount
              objects:
                - name: pods
                  mode: pull
                - name: events
                  mode: watch

          service:
            pipelines:
              logs:
                receivers: [k8sobjects]
                processors: [k8sattributes, resourcedetection/env, batch]
                exporters: *victoriametrics-exporter
              metrics:
                receivers: [k8s_cluster]
                processors: [k8sattributes, resourcedetection/env, batch]
                exporters: *victoriametrics-exporter
              metrics/proxmox:
                receivers: [influxdb]
                processors: [batch]
                exporters: *victoriametrics-exporter

    extraObjects:
      # Add service for external receivers
      - apiVersion: v1
        kind: Service
        metadata:
          name: otel-receivers
          namespace: opentelemetry
          annotations:
            io.cilium/lb-ipam-ips: ${otelcol_lb_ip}
          labels:
            homelab/public-service: "true"
        spec:
          selector:
            app.kubernetes.io/component: opentelemetry-collector
            app.kubernetes.io/instance: opentelemetry.opentelemetry-kube-stack-cluster-stats
          type: LoadBalancer
          sessionAffinity: None
          externalTrafficPolicy: Local
          internalTrafficPolicy: Cluster
          ports:
          - name: influxdb
            protocol: UDP
            port: 8086
            targetPort: 8086
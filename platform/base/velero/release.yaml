apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: velero
  namespace: velero
spec:
  releaseName: velero
  dependsOn:
    # Needs monitoring CRDs as it is required by some options
    - name: prometheus-operator-crds
      namespace: opentelemetry
    - name: snapshot-controller
    - name: s3-instance
      namespace: operators
  chart:
    spec:
      chart: velero
      # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts
      version: "11.1.1" # 10.0.x
      sourceRef:
        kind: HelmRepository
        name: vmware-tanzu
        namespace: flux-system
  interval: 50m
  install:
    remediation:
      retries: 3
  values:
    # https://artifacthub.io/packages/helm/vmware-tanzu/velero?modal=values

    # replicaCount: 2

    # topologySpreadConstraints:
    #   # Maximum 1 pod per Proxmox hypervisor
    #   - maxSkew: 1
    #     topologyKey: topology.kubernetes.io/zone
    #     whenUnsatisfiable: ScheduleAnyway
    #     labelSelector:
    #       matchLabels:
    #         app.kubernetes.io/name: trust-manager
    #         app.kubernetes.io/instance: trust-manager
    #   # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
    #   - maxSkew: 1
    #     topologyKey: kubernetes.io/hostname
    #     whenUnsatisfiable: DoNotSchedule
    #     labelSelector:
    #       matchLabels:
    #         app.kubernetes.io/name: trust-manager
    #         app.kubernetes.io/instance: trust-manager

    initContainers:
      - name: velero-plugin-for-aws
        image: velero/velero-plugin-for-aws:v1.13.0
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - mountPath: /target
            name: plugins
    
    # CRDs
    upgradeCRDs: true
    cleanUpCRDs: false

    # TODO : remove when https://github.com/vmware-tanzu/helm-charts/issues/698 is solved
    kubectl:
      image:
        repository: bitnamilegacy/kubectl
        tag: 1.33.4

    credentials:
      useSecret: true
      name: s3-backup-aws-creds-file # Same name as in external-secrets.yaml

    backupsEnabled: true
    snapshotsEnabled: true

    deployNodeAgent: true

    configuration:
      # https://velero.io/docs/v1.6/api-types/backupstoragelocation/
      backupStorageLocation:
        - name: default
          provider: aws # For s3 storage
          bucket: "${minio_backup_bucket}"

          default: true

          accessMode: ReadWrite
          
          config:
            region: "minio"
            s3ForcePathStyle: true
            s3Url: "http://${minio_url}"
      # https://velero.io/docs/v1.6/api-types/volumesnapshotlocation/
      volumeSnapshotLocation:
        - name: default
          # provider: csi
          # config:
          #   region: ""
          provider: aws # For s3 storage
         
          config:
            region: "minio"
            profile: "default"
            s3ForcePathStyle: true
            s3Url: "http://${minio_url}"

      
      uploaderType: kopia

      features: EnableCSI

      # defaultVolumeSnapshotLocations:

      # Restore externalsecrets resources before secrets otherwise they will be rebuilt
      # TODO : see if we can only backup the externalsecret resource and not both
      restoreResourcePriorities: externalsecrets,secrets

      # defaultVolumesToFsBackup:

      # defaultRepoMaintainFrequency:
    
    # https://velero.io/docs/v1.16/api-types/schedule/
    schedules:
      global:
        disabled: false
        # labels:
        #   myenv: foo
        annotations:
          velero.io/csi-volumesnapshot-class_linstor.csi.linbit.com: "linstor-csi-snapshot-s3"
        schedule: "0 */4 * * *" # 4AM schedule

        useOwnerReferencesInBackup: false
        paused: false
        skipImmediately: false
        template:
          ttl: "24h0m0s" # "240h"
          storageLocation: default
          # https://velero.io/docs/v1.16/resource-filtering/

          uploaderConfig:
            parallelFileUpload: 10

          # Resource selection
          labelSelector:
            matchLabels:
              homelab/backup-resource: "true" # Mark all resources of types included in the list below with this label

          ## Exclude everything by default
          # excludedNamespaces:
          #   - '*'
          # includedNamespaces:
          #   - postgres

          includedResources: &rsc
            - secret
            - externalsecret
            - persistentvolumeclaim # Backuping PVC allows to perform volume snapshot

          # excludedNamespaceScopedResources:
          #   - '*'
          # - persistentVolumeClaims # We backup PVC elsewhere
          # includedNamespaceScopedResources: *rsc
            # - backupStorageLocations
            # - schedules

          # excludedClusterScopedResources:
          # # - persistentVolumes # Alongside PVC
          #   - '*'

          # includedClusterScopedResources: *rsc

          # Volume snapshot
          snapshotVolumes: true
          snapshotMoveData: true
          volumeSnapshotLocations:
            - default
          
          hooks:
            resources: []

      mariadb:
        # Maybe include DB root password here as well
        disabled: false
        annotations:
          velero.io/csi-volumesnapshot-class_linstor.csi.linbit.com: "linstor-csi-snapshot-s3"
        schedule: "0 */4 * * *" # 4AM schedule

        useOwnerReferencesInBackup: false
        paused: false
        skipImmediately: false
        template:
          ttl: "24h0m0s" # "240h"
          storageLocation: default
          # https://velero.io/docs/v1.16/resource-filtering/

          uploaderConfig:
            parallelFileUpload: 10

          # Resource selection
          # labelSelector:
          #   matchLabels:
          #     homelab/backup-mariadb: "true" # Mark PVCs 

          includedNamespaces:
            - mariadb
          includedResources:
            - persistentvolumeclaim # Backuping PVC allows to perform volume snapshot

          # Volume snapshot
          snapshotVolumes: true
          snapshotMoveData: true
          volumeSnapshotLocations:
            - default

          hooks:
            resources:
              - name: mariadb-safe-lock-hook
                # Only run the hook on pod zero
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: mariadb-galera
                    app.kubernetes.io/name: mariadb
                    statefulset.kubernetes.io/pod-name: mariadb-galera-0
                pre:
                  - exec:
                      container: mariadb
                      command:
                        - /bin/sh
                        - -c
                        - mariadb -u root --password=$MARIADB_ROOT_PASSWORD -e "FLUSH TABLES WITH READ LOCK"
                      onError: Fail
                      timeout: 1m
                post:
                  - exec:
                      container: mariadb
                      command:
                        - /bin/sh
                        - -c
                        - mariadb -u root --password=$MARIADB_ROOT_PASSWORD -e "UNLOCK TABLES"
                      onError: Fail
                      timeout: 10s


    extraObjects:
      # S3 bucket
      - apiVersion: s3.onyxia.sh/v1alpha1
        kind: Bucket
        metadata:
          name: "${minio_backup_bucket}-bucket"
          namespace: velero
        spec:
          # Bucket name (on S3 server, as opposed to the name of the CR)
          name: "${minio_backup_bucket}"
          quota:
            default: 100000000000 # 100GB

      # S3 secret mirror
      - apiVersion: external-secrets.io/v1
        kind: ExternalSecret
        metadata:
          name: s3-backup-aws-creds-file
          namespace: velero
        spec:
          data:
            - secretKey: access_key
              remoteRef:
                key: external-minio-secrets
                property: access_key
            - secretKey: secret_key
              remoteRef:
                key: external-minio-secrets
                property: secret_key
          refreshInterval: 5m
          secretStoreRef:
            kind: ClusterSecretStore
            name: external-secrets
          target:
            creationPolicy: Owner
            deletionPolicy: Retain
            template:
              data:
                # https://github.com/vmware-tanzu/velero-plugin-for-aws/blob/main/README.md
                # https://docs.aws.amazon.com/sdkref/latest/guide/file-format.html
                cloud: | # default
                  [default]
                  aws_access_key_id = {{ `{{ .access_key }}` }}
                  aws_secret_access_key = {{ `{{ .secret_key }}` }}
                  region = minio
              engineVersion: v2
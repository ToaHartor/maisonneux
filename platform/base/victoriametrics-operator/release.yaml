apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: victoria-metrics-operator
  namespace: monitoring
spec:
  releaseName: victoria-metrics-operator
  dependsOn:
    - name: victoria-metrics-operator-crds
  chart:
    spec:
      chart: victoria-metrics-operator
      # renovate: datasource=helm depName=victoria-metrics-operator registryUrl=https://victoriametrics.github.io/helm-charts
      version: "0.50.3"
      sourceRef:
        kind: HelmRepository
        name: victoriametrics
        namespace: flux-system
  interval: 50m
  install:
    createNamespace: true
    remediation:
      retries: 3

  values:
    # https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-operator/values.yaml
    crds:
      enabled: false # Installed with dedicated chart

    replicaCount: 2 # One per hypervisor

    watchNamespaces: ["monitoring"] # All resources will be in the same namespace 

    podDisruptionBudget:
      enabled: true
      minAvailable: 1


    affinity:
      podAntiAffinity:
        # Refuse if possible to schedule two pods on the same hypervisor (e.g node)
        # If it does not, they also won't be scheduled on the same pod unless necessary
        preferredDuringSchedulingIgnoredDuringExecution:
          # Anti affinity on the same hypervisor
          - weight: 100
            podAffinityTerm:
              topologyKey: topology.kubernetes.io/zone
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - victoria-metrics-operator
                - key: app.kubernetes.io/instance
                  operator: In
                  values:
                  - victoria-metrics-operator
          # Anti affinity on the same node
          - weight: 50
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - victoria-metrics-operator
                - key: app.kubernetes.io/instance
                  operator: In
                  values:
                  - victoria-metrics-operator
    
    extraObjects:
      # Metrics cluster storage
      - apiVersion: operator.victoriametrics.com/v1beta1
        kind: VMCluster
        metadata:
          name: metrics-datastore
        spec:
          replicationFactor: 1 # 2
          vmstorage:
            replicaCount: 2

            # affinity:
            #   podAntiAffinity:
            #     # Preferrably do not schedule on the same node
            #     preferredDuringSchedulingIgnoredDuringExecution:
            #       # Anti affinity on the same node
            #       - weight: 50
            #         podAffinityTerm:
            #           topologyKey: kubernetes.io/hostname
            #           labelSelector:
            #             matchExpressions:
            #             - key: app.kubernetes.io/name
            #               operator: In
            #               values:
            #               - vmstorage

            topologySpreadConstraints:
              # Maximum 1 pod per Proxmox hypervisor
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vmstorage
              # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vmstorage

            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: ${fastdata_storage}
                  resources:
                    requests:
                      storage: 10Gi
          vmselect:
            replicaCount: 2
            # affinity:
            #   podAntiAffinity:
            #     # Preferrably do not schedule on the same node
            #     preferredDuringSchedulingIgnoredDuringExecution:
            #       # Anti affinity on the same node
            #       - weight: 50
            #         podAffinityTerm:
            #           topologyKey: kubernetes.io/hostname
            #           labelSelector:
            #             matchExpressions:
            #             - key: app.kubernetes.io/name
            #               operator: In
            #               values:
            #               - vmselect
            topologySpreadConstraints:
              # Maximum 1 pod per Proxmox hypervisor
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vmselect
              # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vmselect

          vminsert:
            replicaCount: 2
            # affinity:
            #   podAntiAffinity:
            #     # Preferrably do not schedule on the same node
            #     preferredDuringSchedulingIgnoredDuringExecution:
            #       # Anti affinity on the same node
            #       - weight: 50
            #         podAffinityTerm:
            #           topologyKey: kubernetes.io/hostname
            #           labelSelector:
            #             matchExpressions:
            #             - key: app.kubernetes.io/name
            #               operator: In
            #               values:
            #               - vminsert
            topologySpreadConstraints:
              # Maximum 1 pod per Proxmox hypervisor
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vminsert
              # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vminsert


      # Logs cluster storage
      - apiVersion: operator.victoriametrics.com/v1
        kind: VLCluster
        metadata:
          name: logs-datastore
        spec:
          vlinsert:
            replicaCount: 2
            # affinity:
            #   podAntiAffinity:
            #     # Preferrably do not schedule on the same node
            #     preferredDuringSchedulingIgnoredDuringExecution:
            #       # Anti affinity on the same node
            #       - weight: 50
            #         podAffinityTerm:
            #           topologyKey: kubernetes.io/hostname
            #           labelSelector:
            #             matchExpressions:
            #             - key: app.kubernetes.io/name
            #               operator: In
            #               values:
            #               - vlinsert
            topologySpreadConstraints:
              # Maximum 1 pod per Proxmox hypervisor
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vlinsert
              # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vlinsert


          vlselect:
            replicaCount: 2
            topologySpreadConstraints:
              # Maximum 1 pod per Proxmox hypervisor
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vlselect
              # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vlselect


            # affinity:
            #   podAntiAffinity:
            #     # Preferrably do not schedule on the same node
            #     preferredDuringSchedulingIgnoredDuringExecution:
            #       # Anti affinity on the same node
            #       - weight: 50
            #         podAffinityTerm:
            #           topologyKey: kubernetes.io/hostname
            #           labelSelector:
            #             matchExpressions:
            #             - key: app.kubernetes.io/name
            #               operator: In
            #               values:
            #               - vlselect
          vlstorage:
            retentionPeriod: "4w"
            replicaCount: 2
            topologySpreadConstraints:
              # Maximum 1 pod per Proxmox hypervisor
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vlstorage
              # Maximum 1 pod per node. If only one worker remains, then only one replica is enough
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: vlstorage

            # affinity:
            #   podAntiAffinity:
            #     # Preferrably do not schedule on the same node
            #     preferredDuringSchedulingIgnoredDuringExecution:
            #       # Anti affinity on the same node
            #       - weight: 50
            #         podAffinityTerm:
            #           topologyKey: kubernetes.io/hostname
            #           labelSelector:
            #             matchExpressions:
            #             - key: app.kubernetes.io/name
            #               operator: In
            #               values:
            #               - vlstorage
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: ${fastdata_storage}
                  resources:
                    requests:
                      storage: 10Gi


